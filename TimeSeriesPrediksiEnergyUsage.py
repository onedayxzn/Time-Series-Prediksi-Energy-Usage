# -*- coding: utf-8 -*-
"""Submission2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M31fGUp0H4oLvWF5unCePFRTOWNfLNNZ

#Submission 2   Belajar Pengembangan Machine Learning
Nama : Sukma Ramadhan. A

Import libraries yang akan digunakan
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime

df = pd.read_csv('/content/Energy.csv')
df.head()

df.info()

print(len(df))

df.isnull().sum()

"""Drop Sebagian data"""

df = df.drop(columns=['NOTES','COST','END TIME','TYPE','UNITS'])

df.shape

df.info()

df = df.sort_values(by=['DATE'], axis=0, ascending=True)

df.head()

Dates = df['DATE'].values
Usage = df['USAGE'].values

plt.figure(figsize=(15,5))
plt.plot(Dates, Usage)
plt.title('Energy Usage', fontsize=18)

Dates_train, Dates_test, Usage_train, Usage_test = train_test_split(Dates, Usage, test_size=0.2, train_size=0.8, shuffle=False)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

val_set = windowed_dataset(Usage, window_size=60, batch_size=100, shuffle_buffer=1000)


model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(4),
])

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
                optimizer=optimizer,
                metrics=['mae'])

Mae = (df['USAGE'].max() - df['USAGE'].min())* 0.1
print(Mae)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
     if(logs.get('mae') < 0.1 and logs.get('val_mae') < 0.1):
      print('\n mae < 10%')
      self.model.stop_training = True
callbacks = myCallback()

hist = model.fit(val_set,
                 validation_data= val_set,
                 epochs=100,
                 verbose=2,
                 batch_size=64,
                 callbacks=[callbacks])